---
title: 自适应线性神经网络
date: 2017-06-14 20:36:46


tags: 机器学习
---
***
**本节我们学习另一种单层神经网络：自适应线性神经元(ADAptive	LInear	NEuron,	简称 Adaline)。在Frank	Rosenblatt提出感知计算法不久，Bernard	Widrow和他的博士生Tedd	Hoff 提出了Adaline算法作为感知机的改进算法(B.Widrow	et	al.	Adaptive	"Adaline"	neuron	using chemical	"memistors".)**

**相对于感知机，Adaline算法有趣的多，因为在学习Adaline的过程中涉及到机器学习中一个重 要的概念：定义、最小化损失函数。学习Adaline为以后学习更复杂高端的算法(比如逻辑斯蒂 回归、SVM等)起到抛砖引玉的作用。**

**Adaline和感知机的一个重要区别是Adaline算法中权重参数更新按照线性激活函数而不是单位 阶跃函数。当然，Adaline中激活函数也简单的很。**
 
**虽然Adaline中参数更新不是使用阶跃函数，但是在对测试集样本输出预测类别时还是使用阶 跃函数，毕竟要输出离散值-1,1。**

<div align="center">
![](http://ofhbt8uhx.bkt.clouddn.com/1207849-93a03f14c2401c5c.png)
</div>
<!--more-->
## 使用梯度下降算法最小化损失函数
**在监督机器学习算法中，一个重要的概念就是定义目标函数(objective	function)，而目标函数 就是机器学习算法的学习过程中要优化的目标，目标函数我们常称为损失函数(cost function)，在算法学习(即，参数更新)的过程中就是要最小化损失函数。**

**对于Adaline算法，我们定义损失函数为样本真实值和预测值之间的误差平方和(Sum	of Squared	Erros,	SSE):**
$$J(\omega)=\frac{1}{2}\sum_{i=1}^{n} \lgroup y^{(i)}-\phi \lgroup  z^{(i)} \rgroup \rgroup$$

**寻找最小均方误差就像下山一样，每次算法循环都相当于下降一步，下降一步的歩幅取决于学习率，与图中的权值点的切线斜率相关**

<div align="center">
![](http://ofhbt8uhx.bkt.clouddn.com/1207849-d4695ed1848fce85.png)
</div>

**每次权值逼近均方误差最小点的过程就是梯度下降（Gradient Descent）**

#### 权值更新
$$\omega :=\omega+\Delta \omega$$
#### 权值变化
$$\Delta\omega =-\eta \Delta J \lgroup \omega \rgroup$$

#### $\Delta J \lgroup \omega \rgroup$是代价函数对权值的偏导函数

**福利：	详细的损失函数对权重的偏导数计算过程为**

<div align="center">
![](http://ofhbt8uhx.bkt.clouddn.com/%E6%8D%95%E8%8E%B722.PNG)
</div>

**最终的权值更新公式如下**
<div align="center">
![](http://ofhbt8uhx.bkt.clouddn.com/1207849-ae88a5c0acfa11f9.png)
</div>

## Python实现自适应线性神经元
**既然感知机和Adaline的学习规则非常相似，所以在实现Adaline的时候我们不需要完全重写， 而是在感知机代码基础上进行修改得到Adaline，具体地，我们需要修改fit方法，实现梯度下 降算法:**
```python
#coding:utf-8
'''
Created on 2017年6月14日
自适应神经元（梯度下降法）
@author: 唐杰
'''
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
class AdlineGD(object):
    def __init__(self, eta=0.01, n_iter=50):
        self.eta = eta
        self.n_iter = n_iter
    
    def net_input(self,X):
        return np.dot(X,self.w_[1:])+self.w_[0]
    
    def activation(self,X):
        return self.net_input(X)
    
    def predict(self,X):
        return np.where(self.activation(X)>=0.0,1,-1)
        
    def fit(self,X,y):
        self.w_=np.zeros(1+X.shape[1])
        self.cost_=[]
        for i in range(self.n_iter):
            output=self.net_input(X)
            errors=(y-output)
            self.w_[1:]+=self.eta*X.T.dot(errors)
            self.w_[0]+self.eta*errors.sum()
            cost=(errors**2).sum()/2.0
            self.cost_.append(cost)
        return self    
from numpy import *
from matplotlib.colors import ListedColormap
def plot_decision_region(X,y,classifier,resolution=0.02):
    markers=('s','x','o','^','v')
    colors=('red','blue','lightgreen','gray','cyan')
    cmap=ListedColormap(colors[:len(np.unique(y))])
    
    x1_min,x1_max=X[:,0].min()-1,X[:,0].max()+1
    x2_min,x2_max=X[:,1].min()-1,X[:,1].max()+1
    
    xx1,xx2=np.meshgrid(np.arange(x1_min,x1_max,resolution),
                        np.arange(x2_min,x2_max,resolution))
    Z=classifier.predict(np.array([xx1.ravel(),xx2.ravel()]).T)
    Z=Z.reshape(xx1.shape)
    
    plt.contourf(xx1,xx2,Z,alpha=0.4,cmap=cmap)
    plt.xlim(xx1.min(),xx1.max())
    plt.ylim(xx2.min(),xx2.max())
    
    for idx,cl in enumerate(np.unique(y)):
        plt.scatter(x=X[y==cl,0], y=X[y==cl,1], alpha=0.8, c=cmap(idx), marker=markers[idx], label=cl)


if __name__ == '__main__':
    #pandas读取csv文件，header=none表示原始文件没有列索引需要自己加上  
    df=pd.read_csv('http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data',header=None)
    #print(df.tail())#显示后五条的记录
    y=df.iloc[0:100,4].values#类别标记
    y=np.where(y=='Iris-setosa',-1,1)
    X=df.iloc[0:100,[0,2]].values 
    fig,ax=plt.subplots(nrows=1,ncols=2,figsize=(8,4))
    ada1=AdlineGD(eta=0.01, n_iter=10).fit(X,y)
    ax[0].plot(range(1,len(ada1.cost_)+1),np.log10(ada1.cost_),marker='o')
    ax[0].set_xlabel('Epoches')
    ax[0].set_ylabel('log(Sum-squared-error)')
    ax[0].set_title('Adalie-learning rate 0.01')
    ada2=AdlineGD(eta=0.0001, n_iter=10).fit(X,y)
    ax[1].plot(range(1,len(ada2.cost_)+1),np.log10(ada2.cost_),marker='o')
    ax[1].set_xlabel('Epoches')
    ax[1].set_ylabel('log(Sum-squared-error)')
    ax[1].set_title('Adalie-learning rate 0.0001')
    plt.show()
    
    X_std=np.copy(X)
    X_std[:,0]=(X[:,0]-X[:,0].mean())/X[:,0].std()
    X_std[:,1]=(X[:,1]-X[:,1].mean())/X[:,1].std()
    adal=AdlineGD(eta=0.01, n_iter=15)
    adal.fit(X_std,y)
    
    plot_decision_region(X_std,y,classifier=adal)
    plt.title('Adaline-Gradient Descent')
    plt.xlabel('sepal length [standardized]')
    plt.ylabel('petal length [standardized]')
    plt.legend(loc='upper left')
    plt.show()
    
    plt.plot(range(1,len(adal.cost_)+1),np.log10(adal.cost_),marker='o')
    plt.xlabel('Epoches')
    plt.ylabel('Sum-squared-error')
    plt.title('Adalie-learning rate 0.0001')
    plt.show()
```
### 学习率的影响和选择

**学习率设置为0.01的时候，结果如左图，均方误差最小的点是第一个点，然后越来越大。当学习率设置为0.0001的时候，结果如右图，误差在逐渐减小，但是没有收敛的趋势。**
<div align="center">
![](http://ofhbt8uhx.bkt.clouddn.com/Figure_11111.png)
</div>

**分析上面两幅图各自的问题，左图根本不是在最小化损失函数，反而在每一轮迭代过程中， 损失函数值不断在增大！这说明取值过大的学习率不但对算法毫无益处反而危害大大滴。右 图虽然能够在每一轮迭代过程中一直在减小损失函数的值，但是减小的幅度太小了，估计至 少上百轮迭代才能收敛，而这个时间我们是耗不起的，所以学习率值过小就会导致算法收敛 的时间巨长，使得算法根本不能应用于实际问题。**

**下面左图展示了权重再更新过程中如何得到损失函数 最小值的。右图展示了学习率过大 时权重更新，每次都跳过了最小损失函数对应的权重值。**

<div align="center">
![](http://ofhbt8uhx.bkt.clouddn.com/1222.PNG)
</div>

**许多机器学习算法都要求先对特征进行某种缩放操作，比如标准化(standardization)和归一化 (normalization)。而缩放后的特征通常更有助于算法收敛，实际上，对特征缩放后在运用梯度 下降算法往往会有更好的学习效果。**

**特征标准化的计算很简单，比如要对第j维度特征进行标准化，只需要计算所有训练集样本中 第j维度的平均值 和标准差 即可,然后套公式：**
$$x\prime_j= \frac{x_j-\mu_j}{\sigma_j}$$
**标准化后的特征	均值为0，标准差为1. **

**经过标准化的数据，会体现出一些数学分布的特点。标准化后，我们再次使用0.01的学习率进行训练分类。**
<div align="center">
![](http://ofhbt8uhx.bkt.clouddn.com/Figure_33333.png)
</div>
**最后的分类平面如下图**
<div align="center">
![](http://ofhbt8uhx.bkt.clouddn.com/Figure_2222.png)
</div>

## 大规模机器学习和随机梯度下降
**虽然随机梯度下降被当作是梯度下降的近似算法，但实际上她往往比梯度下降收敛更快，因 为相同时间内她对权重更新的更频繁。由于单个样本得到的损失函数相对于用整个训练集得 到的损失函数具有随机性，反而会有助于随机梯度下降算法避免陷入局部最小点。在实际应 用随机梯度下降法时，为了得到准确结果，一定要以随机方式选择样本计算梯度，通常的做 法在每一轮迭代后将训练集进行打乱重排(shuffle)。**

**Notes:在随机梯度下降法中，通常用不断减小的自适应学习率替代固定学习率 ,比如 ,其 中 是常数。还要注意随机梯度下降并不能够保证使损失函数达到全局最小点，但结果会 很接近全局最小。**

**随机梯度下降法的另一个优点是可以用于在线学习(online	learning)。在线学习在解决不断累 积的大规模数据时非常有用，比如，移动端的顾客数据。使用在线学习，系统可以实时更新 并且如果存储空间快装不下数据了，可以将时间最久的数据删除。**

**Notes	除了梯度下降算法和随机梯度下降算法之外，还有一种常用的二者折中的算法：最小 批学习(mini-batch	learning)。很好理解，梯度下降每一次用全部训练集计算梯度更新权重， 随机梯度法每一次用一个训练样本计算梯度更新权重，最小批学习每次用部分训练样本计算 梯度更新权重，比如50。相对于梯度下降，最小批收敛速度也更快因为权重参数更新更加频 繁。此外，最小批相对于随机梯度中，使用向量操作替代for循环(每一次跌倒都要遍历所有样 本)，使得计算更快。**

**上一节我们已经实现了梯度下降求解Adaline，只需要做部分修改就能得到随机梯度下降法求 解Adaline。第一个修改是fit方法内用每一个训练样本更新权重参数 ,第二个修改是增加 partial_fit方法，第三个修改是增加shuffle方法打乱训练集顺序。**

```python
#coding:utf-8
'''
Created on 2017年6月14日
自适应神经元（随机梯度下降法）
@author: 唐杰
'''
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from numpy.random import seed
class AdalineSGD(object):
    def __init__(self, eta=0.01, n_iter=10,shuffle=True,random_state=None):
        self.eta = eta
        self.n_iter = n_iter
        self.w_initialized=False  
        self.shuffle=shuffle              
        if random_state:                         
            seed(random_state)
    
    #洗牌训练集
    def _shuffle(self,X,y):            
        r=np.random.permutation(len(y))      
        return X[r],y[r]   
    #初始化为0的权重
    def _initialize_weights(self,m):    
        self.w_=np.zeros(1+m)  
        self.w_initialized=True        
    
    def _update_weights(self,xi,target):   
        output=self.net_input(xi)    
        error=(target-output)                                
        self.w_[1:]+=self.eta*xi.dot(error)     
        self.w_[0]+=self.eta*error    
        cost=0.5*error**2          
        return cost
            
    def net_input(self,X):
        return np.dot(X,self.w_[1:])+self.w_[0]
    
    def activation(self,X):
        return self.net_input(X)
    
    def predict(self,X):
        return np.where(self.activation(X)>=0.0,1,-1)
        
    def fit(self,X,y):
        self._initialize_weights(X.shape[1])#初始化权重 self.w_=np.zeros(1+X.shape[1])        
        self.cost_=[]
        for i in range(self.n_iter):
            if self.shuffle:
                X,y=self._shuffle(X, y)#洗牌
            cost=[]
            for xi,target in zip(X,y):
                cost.append(self._update_weights(xi,target))  
            avg_cost=sum(cost)/len(y) 
            self.cost_.append(avg_cost)  
        return self
    #未初始化权重的训练 
    def partial_fit(self,X,y):     
        if not self.w_initialized: 
            self._initialize_weights(X.shape[1])
        if y.ravel().shape[0]>1:       
            for xi,target in zip(X,y):  
                self._update_weights(xi,target)   
        else:        
            self._update_weights(X,y)        
        return self
    
    
    
from numpy import *
from matplotlib.colors import ListedColormap
def plot_decision_region(X,y,classifier,resolution=0.02):
    markers=('s','x','o','^','v')
    colors=('red','blue','lightgreen','gray','cyan')
    cmap=ListedColormap(colors[:len(np.unique(y))])
    
    x1_min,x1_max=X[:,0].min()-1,X[:,0].max()+1
    x2_min,x2_max=X[:,1].min()-1,X[:,1].max()+1
    
    xx1,xx2=np.meshgrid(np.arange(x1_min,x1_max,resolution),
                        np.arange(x2_min,x2_max,resolution))
    Z=classifier.predict(np.array([xx1.ravel(),xx2.ravel()]).T)
    Z=Z.reshape(xx1.shape)
    
    plt.contourf(xx1,xx2,Z,alpha=0.4,cmap=cmap)
    plt.xlim(xx1.min(),xx1.max())
    plt.ylim(xx2.min(),xx2.max())
    
    for idx,cl in enumerate(np.unique(y)):
        plt.scatter(x=X[y==cl,0], y=X[y==cl,1], alpha=0.8, c=cmap(idx), marker=markers[idx], label=cl)


if __name__ == '__main__':
    #pandas读取csv文件，header=none表示原始文件没有列索引需要自己加上  
    df=pd.read_csv('http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data',header=None)
    #print(df.tail())#显示后五条的记录
    y=df.iloc[0:100,4].values#类别标记
    y=np.where(y=='Iris-setosa',-1,1)
    X=df.iloc[0:100,[0,2]].values 
#    fig,ax=plt.subplots(nrows=1,ncols=2,figsize=(8,4))
#    ada1=AdlineGD(eta=0.01, n_iter=10).fit(X,y)
#    ax[0].plot(range(1,len(ada1.cost_)+1),np.log10(ada1.cost_),marker='o')
#    ax[0].set_xlabel('Epoches')
#    ax[0].set_ylabel('log(Sum-squared-error)')
#    ax[0].set_title('Adalie-learning rate 0.01')
#    ada2=AdlineGD(eta=0.0001, n_iter=10).fit(X,y)
#    ax[1].plot(range(1,len(ada2.cost_)+1),np.log10(ada2.cost_),marker='o')
#    ax[1].set_xlabel('Epoches')
#    ax[1].set_ylabel('log(Sum-squared-error)')
#    ax[1].set_title('Adalie-learning rate 0.0001')
#    plt.show()
    
    X_std=np.copy(X)
    X_std[:,0]=(X[:,0]-X[:,0].mean())/X[:,0].std()
    X_std[:,1]=(X[:,1]-X[:,1].mean())/X[:,1].std()
    adal=AdalineSGD(eta=0.01, n_iter=15,random_state=1)
    adal.fit(X_std,y)
    
    plot_decision_region(X_std,y,classifier=adal)
    plt.title('Adaline-Stochastic Gradient Descent')
    plt.xlabel('sepal length [standardized]')
    plt.ylabel('petal length [standardized]')
    plt.legend(loc='upper left')
    plt.show()
    
    plt.plot(range(1,len(adal.cost_)+1),adal.cost_,marker='o')
    plt.xlabel('Epoches')
    plt.ylabel('Average Cost')
    plt.title('Adalie-learning rate 0.01')
    plt.show()
```
**得到的结果为：**

**经过标准化的数据，会体现出一些数学分布的特点。标准化后，我们再次使用0.01的学习率进行训练分类。**
<div align="center">
![](http://ofhbt8uhx.bkt.clouddn.com/Figure_11423.png)
</div>
**最后的分类平面如下图:**
<div align="center">
![](http://ofhbt8uhx.bkt.clouddn.com/Figure_122.png)
</div>